{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6347161",
   "metadata": {},
   "source": [
    "\\# Lab 5. Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e371842e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "How to Use This Notebook\n",
    "---\n",
    "\n",
    "**Recommended Setup**\n",
    "- For the best experience, **run this notebook on [Google Colab](https://colab.research.google.com/)**—especially if your local machine is slow.  \n",
    "- In Colab, **enable GPU support** by going to:  \n",
    "  `Runtime > Change runtime type > Hardware accelerator > GPU`\n",
    "\n",
    "\n",
    "**Homework Tasks**\n",
    "\n",
    " - Homework tasks are clearly marked throughout the notebook in the following format:\n",
    "\n",
    "   > ---\n",
    "\n",
    "   > <span style=\"color:red\"><b>TASK X</b> - [<i>some text</i>]:</span>\n",
    "\n",
    "   > ---\n",
    "\n",
    "   > ```Your code ....```\n",
    "\n",
    "   > ---\n",
    "\n",
    "   > *End of Task X.* [*Instructions for passing*]\n",
    "\n",
    " - For each task:\n",
    "   - **Complete the code** where indicated.\n",
    "   - **Upload the required results** from each task to **Homework 5 – Code** on [NextIlearn](https://nextilearn.dsv.su.se).\n",
    "\n",
    " - Once you've finished all the tasks:\n",
    "   Submit your **entire completed notebook (including your code!)** to **Homework 5 – Notebook** on [NextIlearn](https://nextilearn.dsv.su.se).\n",
    "\n",
    "**Important:**  \n",
    "Your submission will **only be graded if both files** (code + notebook) are uploaded **before the deadline**. Late submissions are **not accepted**, regardless of technical issues like bad internet connection.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c2e59e",
   "metadata": {},
   "source": [
    "This lab introduces students to working with pre-trained BERT embeddings, fine-tuning BERT for classification, and extending BERT with a simple autoregressive head.\n",
    "\n",
    "\\#\\# Objectives\n",
    "- Extract contextual embeddings using a pre-trained BERT model.\n",
    "- Fine-tune BERT on a downstream classification task.\n",
    "- Build a simple autoregressive extension of BERT.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install necessary libraries\n",
    "!pip install transformers torch scikit-learn datasets matplotlib\n",
    "\n",
    "# Unzip data\n",
    "#!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eef6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa7829",
   "metadata": {},
   "source": [
    "\\#\\# 1. Using Pre-trained BERT Embeddings\n",
    "\n",
    "Let's load a pre-trained BERT model and extract token-level and sentence-level embeddings. [Huggingface's `transformers` library](https://huggingface.co/transformers]) provides easy access to most open source transformer models. The most convenient way is using their `Auto`-classes, which work with most models:\n",
    "\n",
    "* `transformers.AutoTokenizer` provides access to the pretrained tokenizer (i.e. including the vocabulary used for training the model)\n",
    "* `transformers.AutoModel` provides access to the pretrained model in its base configuration (i.e. including the trained parameters)\n",
    "\n",
    "Both provide the `.from\\_pretrained()` method, which automatically instantiates the correct class and downloads the pretrained data based on a model identifier:\n",
    "\n",
    "*   `'bert-base-uncased'`: normal sized BERT ([Devlin et al, 2019](https://doi.org/10.48550/arXiv.1810.04805))\n",
    "*   `'bert-small-uncased'`: a smaller version of BERT ([Devlin et al, 2019](https://doi.org/10.48550/arXiv.1810.04805))\n",
    "*   `'FacebookAI/xlm-roberta-base'`: a BERT-sized multilingual model ([Conneau et al, 2019](https://doi.org/10.48550/arXiv.1911.02116))\n",
    "*   `'gpt2'`: a predecessor of today's GPT-4o, the backbone of ChatGPT ([Radford et al, 2019](https://cdn.openai.com/better-language-models/language\\_models\\_are\\_unsupervised\\_multitask\\_learners.pdf))\n",
    "\n",
    "An extensive list of models can be found at [https://huggingface.co/models](https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cfe3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72756ac",
   "metadata": {},
   "source": [
    "\\#\\#\\# **1. Step:** Load the pretrained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb90f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b5ffa",
   "metadata": {},
   "source": [
    "Let's encode some sample sentences to see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6efe02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"The quick brown fox jumps over the lazy dog.\", \"A stitch in time saves nine.\"]\n",
    "\n",
    "# tokenize the sentences:\n",
    "inputs = tokenizer(sentences,\n",
    "  return_tensors='pt',    # return the output of this function as pytorch tensors.\n",
    "                          # Other options: 'np' -> numpy\n",
    "                          #                'tf' -> tensorflow\n",
    "\n",
    "  padding='max_length',   # pad the sentences to context length of the model.\n",
    "                          # Other options: 'longest' / True     -> pad to longest length in batch\n",
    "                          #                'do_not_pad' / False -> no padding\n",
    "\n",
    "  truncation=True         # Options: 'longest_first' / True    -> Truncate to a maximum length specified with the argument max_length or to the maximum acceptable input length for the model if that argument is not provided.\n",
    "                          #          'do_not_truncate' / False -> No truncation (i.e., can output batch with sequence lengths greater than the model maximum admissible input size)\n",
    ")\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4787d508",
   "metadata": {},
   "source": [
    "`inputs` is a dictionary-like object including all the information needed for the transformer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874c2a59",
   "metadata": {},
   "source": [
    "\\#\\#\\#\\# **Member `'input\\_ids'`**: the actual tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef49f14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in encoded form:\n",
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a264e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shape is number of input texts x sequnece length:\n",
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013c90f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be converted back to text:\n",
    "[tokenizer.decode(ids) for ids in inputs['input_ids']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca030945",
   "metadata": {},
   "source": [
    "[huggingface tokenizer objects](https://huggingface.co/docs/transformers/main\\_classes/tokenizer) contain information on the model specific special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d1bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15dfa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unknown token -> Encodes tokens that have not occured in the training data\n",
    "tokenizer.unk_token, tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a87f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification token -> Starts a sequence. Because of this, other transformers\n",
    "# usually use \"beginning of sequence\" ([bos]) instead. For BERT-like models\n",
    "# it corresponds to the position of the classification output.\n",
    "tokenizer.cls_token, tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separator token -> Separates two sentences for the next sentence prediction task\n",
    "# after pretraining usually used to end the input sequence. Because of this,\n",
    "# other transformers usually use \"end of sequence\" ([eos]) instead.\n",
    "tokenizer.sep_token, tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ae1698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding token -> Pads sequences to the full input length of the transformer.\n",
    "tokenizer.pad_token, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a49e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask token -> For the masked language modelling pretraining task. Rarelly used\n",
    "# after pretraining.\n",
    "tokenizer.mask_token, tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625b2232",
   "metadata": {},
   "source": [
    "\\#\\#\\#\\# **Member `'attention\\_mask'`**: a mask specifing the position of  non-padded input tokens in `'input\\_ids'`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc01c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cebff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ea17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tokenizer.decode(ids[mask == 1]) for ids, mask in zip(inputs['input_ids'], inputs['attention_mask'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf63333",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tokenizer.decode(ids[mask == 0]) for ids, mask in zip(inputs['input_ids'], inputs['attention_mask'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1133b29",
   "metadata": {},
   "source": [
    "\\#\\#\\#\\# **Member `'token\\_type\\_ids'`**: a mask specifing the position of the two sentences in `'input\\_ids'` for the next sentence prediction pretraining task!\n",
    "\n",
    "*Usually not needed after pretraining!*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8b55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36adc3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['token_type_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecd101",
   "metadata": {},
   "source": [
    "\\#\\#\\# **2. Step:** Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb6ca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205b29a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model:\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07937a45",
   "metadata": {},
   "source": [
    "The `config` property contains information about the model instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34382a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe145060",
   "metadata": {},
   "source": [
    "We can use this model rightaway, since it is already pretrained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80550ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  outputs = model(\n",
    "      input_ids=inputs['input_ids'].to(device),\n",
    "      attention_mask=inputs['attention_mask'].to(device),\n",
    "      output_hidden_states=True,                # return the hidden states after each transformer layer (default: False)\n",
    "      output_attentions=True                    # return the self-attention weights (default: False)\n",
    "  )\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4572db7",
   "metadata": {},
   "source": [
    "`outputs` is again a dictionary-like object:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0461880",
   "metadata": {},
   "source": [
    "\\#\\#\\#\\# **Member `'last\\_hidden\\_state'`**: the output of the last self-attention layer (i.e. before pooling)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad11a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['last_hidden_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc5617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1866edca",
   "metadata": {},
   "source": [
    "\\#\\#\\#\\# **Member `'pooler\\_output'`**: the output of the pooling layer!\n",
    "\n",
    "The pooler is a single linear layer with a tanh activation:\n",
    "\n",
    "```Python\n",
    "class BertPooler(nn.Module):\n",
    "    def \\_\\_init\\_\\_(self, config):\n",
    "        super().\\_\\_init\\_\\_()\n",
    "        self.dense = nn.Linear(config.hidden\\_size, config.hidden\\_size)\n",
    "        self.activation = nn.Tanh()\n",
    "    \n",
    "    def forward(self, hidden\\_states):\n",
    "        \\# We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        \\# to the first token.\n",
    "        first\\_token\\_tensor = hidden\\_states[:, 0]\n",
    "        pooled\\_output = self.dense(first\\_token\\_tensor)\n",
    "        pooled\\_output = self.activation(pooled\\_output)\n",
    "        return pooled\\_output\n",
    "```\n",
    "\n",
    "This layer recieves the last hidden state corresponding to the `[CLS]` token as the input. Remember that the next sentence prediction task is trained on this output for BERT.\n",
    "\n",
    "(see [BERT source code](https://huggingface.co/transformers/v3.0.2/\\_modules/transformers/modeling\\_bert.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a9bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['pooler_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ddc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs['pooler_output'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65da0ef1",
   "metadata": {},
   "source": [
    "\\#\\#\\#\\# **Member `'hidden\\_states'`**: all the hidden states after each transformer layer!\n",
    "\n",
    "*This is only returned because we specified* `output\\_hidden\\_states=True`*!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69932220",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(outputs['hidden_states']), len(outputs['hidden_states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf0403",
   "metadata": {},
   "outputs": [],
   "source": [
    "[t.shape for t in outputs['hidden_states']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afce1f64",
   "metadata": {},
   "source": [
    "\\#\\#\\#\\# **Member `'attentions'`**: all the attention weights of each head in each transformer layer!\n",
    "\n",
    "*This is only returned because we specified* `output\\_attentions=True`*!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f10135",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(outputs['attentions']), len(outputs['attentions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078b9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "[t.shape for t in outputs['attentions']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6324dc81",
   "metadata": {},
   "source": [
    "Let's have a look at the average attention in the last layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1983779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(2):\n",
    "  fig, axs = plt.subplots(ncols=12, figsize=(20, 5))\n",
    "  for layer, ax in enumerate(axs):\n",
    "    # get attention weights of last transformer layer:\n",
    "    aw = outputs['attentions'][layer][i].cpu()\n",
    "\n",
    "    # average over heads:\n",
    "    aw = aw.mean(dim=0)\n",
    "\n",
    "    # remove padding tokens:\n",
    "    mask = inputs['attention_mask'][i]\n",
    "    aw = aw[mask == 1, :][:, mask == 1]\n",
    "\n",
    "    # create labels:\n",
    "    labels = tokenizer.convert_ids_to_tokens(inputs['input_ids'][i][mask == 1])\n",
    "    x = np.arange(len(labels))\n",
    "\n",
    "    ax.imshow(aw.detach().numpy())\n",
    "    ax.set_xticks(ticks=x, labels=labels, rotation=90)\n",
    "    ax.set_yticks(ticks=x, labels=['']*len(x))\n",
    "    ax.set_title(f'Layer {layer+1}')\n",
    "\n",
    "  axs[0].set_yticks(ticks=x, labels=labels)\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d91194a",
   "metadata": {},
   "source": [
    "\\#\\#\\# BERT 模型一般会返回什么内容呢？\n",
    "\n",
    "outputs.last\\_hidden\\_state       ✅ 每个 token 的向量（最常用）\n",
    "\n",
    "\n",
    "outputs.pooler\\_output           ✅ 整句的向量（做分类时用）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3272dae",
   "metadata": {},
   "source": [
    "Note how every attention layer focuses on different token combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e677083",
   "metadata": {},
   "source": [
    "\\#\\#\\# Example: using pretrained models as embeddings without furter fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdf494e",
   "metadata": {},
   "source": [
    "Let's load a dataset:\n",
    "\n",
    "From (https://paperswithcode.com/dataset/ag-news)[https://paperswithcode.com/dataset/ag-news]:\n",
    "> *AG News (AG’s News Corpus) is a subdataset of AG's corpus of news articles constructed by assembling titles and description fields of articles from the 4 largest classes (“World”, “Sports”, “Business”, “Sci/Tech”) of AG’s Corpus. The AG News contains 30,000 training and 1,900 test samples per class.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625558fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "labels = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
    "data = load_dataset(\"ag_news\", split='train[:2000]')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa99998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aef0df6",
   "metadata": {},
   "source": [
    "Add a tokenization step to the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1cc436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    #这里的tokenizer会自动给我的结果加上'input_ids', 'token_type_ids', 'attention_mask'\n",
    "    #其中'input_ids'是我的上下文向量\n",
    "    #'token_type_ids'是可选的，token数据类型\n",
    "    #'attention_mask'注意力标记，哪些向量应该被注意\n",
    "    return tokenizer(example['text'], truncation=True, padding='max_length')\n",
    "\n",
    "data = data.map(tokenize_function, batched=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a920c",
   "metadata": {},
   "source": [
    "Hide irrelevant features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b447cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert需要'input_ids','attention_mask', 'label'\n",
    "data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e271ec67",
   "metadata": {},
   "source": [
    "Calculate BERT embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a537a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = [], []\n",
    "\n",
    "# Hugging Face的所有数据集都实现了Dataloader\n",
    "\n",
    "# predict:\n",
    "for batch in tqdm(DataLoader(data, batch_size=32)):\n",
    "  with torch.no_grad():\n",
    "    y.extend(batch.pop('label').cpu().numpy())\n",
    "    #items()返回一个可迭代对象包含每一对键和值\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    #model(**batch) ==  model(batch['input_ids'],batch[\"attention_mask\"])\n",
    "    x.extend(model(**batch).pooler_output.cpu().numpy())\n",
    "\n",
    "# convert lists to numpy:\n",
    "x = np.array(x)\n",
    "y = np.array(y)\n",
    "\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b44ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_knn, f1_rnd = [], []\n",
    "ks = np.arange(1, 21)\n",
    "for k in tqdm(ks):\n",
    "  f1_knn.append([])\n",
    "  f1_rnd.append([])\n",
    "\n",
    "  # Monte-Carlo cross-validation with 50 splits:\n",
    "  mc = ShuffleSplit(n_splits=50, test_size=0.25, train_size=None)\n",
    "  for idx_train, idx_test in mc.split(x):\n",
    "\n",
    "    # knn classifier:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k).fit(x[idx_train], y[idx_train])\n",
    "    y_pred = knn.predict(x[idx_test])\n",
    "    f1_knn[-1].append(f1_score(y[idx_test], y_pred, average='macro'))\n",
    "\n",
    "    # random baseline:\n",
    "    y_rnd = np.random.randint(0, len(labels), size=len(y[idx_test]))\n",
    "    f1_rnd[-1].append(f1_score(y[idx_test], y_rnd, average='macro'))\n",
    "\n",
    "# convert to numpy:\n",
    "f1_knn = np.array(f1_knn)\n",
    "f1_rnd = np.array(f1_rnd)\n",
    "\n",
    "# plot:\n",
    "plt.plot(np.mean(f1_knn, axis=1), label='KNN')\n",
    "plt.plot(np.mean(f1_rnd, axis=1), label='Random')\n",
    "plt.legend()\n",
    "plt.xticks(ticks=ks-1, labels=ks)\n",
    "plt.xlabel('$k$')\n",
    "plt.ylabel('F$_1$-score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4d3aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b676df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<span style=\"color:red\"><b>TASK 1</b> - Text similarity:</span>\n",
    "\n",
    "---\n",
    "\n",
    "Write a function to compute text similarity using cosine similarity between BERT embeddings of different texts. The function should take a list of $n$ strings as its input and return a similarity matrix $\\in \\mathbb\\{R\\}^\\{n \\times n\\}$.\n",
    "\n",
    "Afterward, use this function to compute the similarity for the sentences in `data.zip/task1/sentences.csv` and upload your solution to NextIlearn.\n",
    "\n",
    "**Hint:** See [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine\\_similarity.html) for documentation of the `sklearn.metrics.pairwise.cosine\\_similarity` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521297f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_similarity(texts:Iterable[str], model:BertModel, tokenizer:BertTokenizer, device:torch.device) -> NDArray[np.float32]:\n",
    "  # do stuff here\n",
    "    model.eval()\n",
    "    inputs = tokenizer(texts, padding= True, truncation= True, return_tensors= \"pt\")\n",
    "    inputs = {k : v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.pooler_output\n",
    "\n",
    "    embeddings = embeddings.cpu().numpy()\n",
    "    similarity = cosine_similarity(embeddings).astype(np.float32)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d517c532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate similarity for sample sentences:\n",
    "sentences  = pd.read_csv('data/task1/sentences.csv', index_col=0)\n",
    "similarity = text_similarity(sentences['sentences'].values.tolist(), model, tokenizer, device)\n",
    "pd.DataFrame(similarity).to_csv('similarity.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf8d3d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*End of Task 1. Upload your final predictions (the file* `similarity.csv` *) to* **Homework 2 - Code** *on* **NextIlearn**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe94bde",
   "metadata": {},
   "source": [
    "When you are done with Task 1, feel free to play around with the function a little:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7768f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(text_similarity(['Paul is cooking dinner for his friend.', 'Maria is cooking dinner for her friend.', 'Stockholm is a beautiful city!'], model, tokenizer, device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03ad3c5",
   "metadata": {},
   "source": [
    "\\#\\# 2. Fine-tuning BERT for Text Classification\n",
    "\n",
    "While the KNN classifier based on BERT embeddings is already performing well, we can improve on them by fine-tuning the model for our task. We will reuse the data preprocessing pipeline we used before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data in training and test set:\n",
    "split_data = data.train_test_split(test_size=0.25)\n",
    "split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaff010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders:\n",
    "train_dataloader = DataLoader(split_data['train'], batch_size=16)\n",
    "test_dataloader = DataLoader(split_data['test'], batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ac551b",
   "metadata": {},
   "source": [
    "For classification, we need a different \"head\" on our model. Huggingface provides different setups for each model (see [huggingface documentation](https://huggingface.co/transformers/v3.0.2/model\\_doc/auto.html)):\n",
    "\n",
    "*   `transformers.AutoModel`\n",
    "*   `transformers.AutoModelForPreTraining`\n",
    "*   `transformers.AutoModelWithLMHead`\n",
    "*   `transformers.AutoModelForSequenceClassification`\n",
    "*   `transformers.AutoModelForQuestionAnswering`\n",
    "*   `transformers.AutoModelForTokenClassification`\n",
    "\n",
    "For text classification we use `transformers.AutoModelForSequenceClassification`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131ee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(labels))\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da172e61",
   "metadata": {},
   "source": [
    "As you can see, this model has an **additional linear layer** after the pooler layer. It also comes with built-in loss calculation for convenience:\n",
    "\n",
    "```Python\n",
    "loss = None\n",
    "if labels is not None:\n",
    "    if self.config.problem\\_type is None:\n",
    "        if self.num\\_labels == 1:\n",
    "            self.config.problem\\_type = \"regression\"\n",
    "        elif self.num\\_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "            self.config.problem\\_type = \"single\\_label\\_classification\"\n",
    "        else:\n",
    "            self.config.problem\\_type = \"multi\\_label\\_classification\"\n",
    "\n",
    "    if self.config.problem\\_type == \"regression\":\n",
    "        loss\\_fct = MSELoss()\n",
    "        if self.num\\_labels == 1:\n",
    "            loss = loss\\_fct(logits.squeeze(), labels.squeeze())\n",
    "        else:\n",
    "            loss = loss\\_fct(logits, labels)\n",
    "    elif self.config.problem\\_type == \"single\\_label\\_classification\":\n",
    "        loss\\_fct = CrossEntropyLoss()\n",
    "        loss = loss\\_fct(logits.view(-1, self.num\\_labels), labels.view(-1))\n",
    "    elif self.config.problem\\_type == \"multi\\_label\\_classification\":\n",
    "        loss\\_fct = BCEWithLogitsLoss()\n",
    "        loss = loss\\_fct(logits, labels)\n",
    "```\n",
    "(*from* [https://github.com/huggingface/transformers/blob/v4.51.3/src/transformers/models/bert/modeling\\_bert.py](https://github.com/huggingface/transformers/blob/v4.51.3/src/transformers/models/bert/modeling\\_bert.py\\#L1692))\n",
    "\n",
    "\n",
    "Let's try this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c17b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(\n",
    "  input_ids      = data[:1]['input_ids'].to(device),\n",
    "  attention_mask = data[:1]['attention_mask'].to(device),\n",
    "  labels         = data[:1]['label'].to(device)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bc338b",
   "metadata": {},
   "source": [
    "Now let's train this model. Remember, that **in fine-tuning we want to update the pretrained weights, not re-train the model**. Therefore, we train the model for a low number of epochs with a low learning rate. The original BERT paper proposes the following configuration ([Devlin et al, 2019](https://doi.org/10.48550/arXiv.1810.04805)):\n",
    "\n",
    "* **Learning rate with Adam:** *5e-5, 3e-5, 2e-5*\n",
    "* **Number of epochs:** *2, 3, 4*\n",
    "\n",
    "Further reading: *Huggingface also provides a* `transformers.Trainer`*-class that you may want to try out for convenience: [https://huggingface.co/docs/transformers/main\\_classes/trainer](https://huggingface.co/docs/transformers/main\\_classes/trainer)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f2d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "import tqdm\n",
    "\n",
    "# Optimizer:\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop:\n",
    "loss_train = []\n",
    "for epoch in range(2):\n",
    "\n",
    "  model.train()\n",
    "  loss_train.append([])\n",
    "  for batch in tqdm.tqdm(train_dataloader, desc= f\" epoch {epoch + 1}\"):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(\n",
    "      input_ids      = batch['input_ids'].to(device),\n",
    "      attention_mask = batch['attention_mask'].to(device),\n",
    "      labels         = batch['label'].to(device)\n",
    "    )\n",
    "\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    loss_train[-1].append(loss.item())\n",
    "\n",
    "    optimizer.step()\n",
    "  loss_train[-1] = np.mean(loss_train[-1])\n",
    "\n",
    "  print(f\"Epoch {epoch+1}: loss = {loss_train[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e67331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation:\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "for batch in test_dataloader:\n",
    "  with torch.no_grad():\n",
    "    outputs = model(\n",
    "      input_ids      = batch['input_ids'].to(device),\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "    )\n",
    "\n",
    "  y_pred.extend(outputs.logits.argmax(dim=1).cpu().numpy())\n",
    "  y_true.extend(batch['label'].cpu().numpy())\n",
    "\n",
    "f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3917ef15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<span style=\"color:red\"><b>TASK 2</b> - Fine-Tuning RoBERTa:</span>\n",
    "\n",
    "---\n",
    "\n",
    "Fine-tune [RoBERTa](https://huggingface.co/docs/transformers/model\\_doc/roberta) a pretrained transformer model to classify the data you received along with this notebook (in `data.zip/task2/...`). **You are enouraged to use scheduling and early stopping, but remember to keep the learning rate low.**\n",
    "\n",
    "**Upload the resulting predictions to NextIlearn. Your model should achieve an F$\\_1$ > .77 to pass.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4254efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labeled list of training files:\n",
    "train_files = pd.read_csv('data/task2/train/labels.csv', index_col=0)\n",
    "train_files['file'] = ['data/task2/train/' + s for s in train_files['file']]\n",
    "print(f'# of positive samples: {(train_files.label == 1).sum():d}')\n",
    "print(f'# of negative samples: {(train_files.label == 0).sum():d}')\n",
    "train_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb7f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data sample:\n",
    "with open(train_files.file.sample(1).iloc[0], 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af978fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load list of test files:\n",
    "import os\n",
    "test_files = ['data/task2/test/' + s for s in os.listdir('data/task2/test/')]\n",
    "test_files.sort()\n",
    "test_files = pd.DataFrame({'file': test_files})\n",
    "test_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b1d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "import tqdm\n",
    "\n",
    "\n",
    "class Task2Dataset(Dataset):\n",
    "    def __init__(self, label_file, data_file, tokenizer, max_length=128):\n",
    "        self.labels = pd.read_csv(label_file)\n",
    "        self.data_file = data_file\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.labels.iloc[idx]\n",
    "        file_name = row[\"file\"]\n",
    "        file_label = row[\"label\"]\n",
    "        file_text_path = os.path.join(self.data_file, file_name)\n",
    "        with open(file_text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            file_text = f.read().strip()\n",
    "        return file_text, file_label\n",
    "\n",
    "\n",
    "def collate_fn(batch, tokenizer, max_length=256):\n",
    "    texts, labels = zip(*batch)\n",
    "    encoded = tokenizer(\n",
    "        list(texts),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**batch).logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            y_pred.extend(preds.cpu().tolist())\n",
    "            y_true.extend(batch[\"labels\"].cpu().tolist())\n",
    "    return f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "\n",
    "label_file = r\"D:\\pythonProject\\Lab5_empty\\data\\task2\\train\\labels.csv\"\n",
    "data_file = r\"D:\\pythonProject\\Lab5_empty\\data\\task2\\train\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Split into train/val CSVs\n",
    "df = pd.read_csv(label_file)\n",
    "df_train, df_val = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "df_train.to_csv(\"train_split.csv\", index=False)\n",
    "df_val.to_csv(\"val_split.csv\", index=False)\n",
    "\n",
    "\n",
    "train_dataset = Task2Dataset(\"train_split.csv\", data_file, tokenizer)\n",
    "val_dataset = Task2Dataset(\"val_split.csv\", data_file, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True,\n",
    "                          collate_fn=partial(collate_fn, tokenizer=tokenizer),\n",
    "                          )\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False,\n",
    "                        collate_fn=partial(collate_fn, tokenizer=tokenizer),\n",
    "                        )\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "num_epochs = 8\n",
    "best_f1 = 0.0\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, num_batches = 0, 0\n",
    "\n",
    "    for batch in tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    mean_loss = total_loss / num_batches\n",
    "    print(f\"\\n Epoch {epoch+1} finished. Mean Loss: {mean_loss:.4f}\")\n",
    "\n",
    "    val_f1 = evaluate(model, val_loader, device)\n",
    "    print(f\" Validation F1-score: {val_f1:.4f}\\n\")\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        model.save_pretrained(\"best_model\")\n",
    "        tokenizer.save_pretrained(\"best_model\")\n",
    "        print(\"Saved new best model!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1900a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_task2 = AutoModelForSequenceClassification.from_pretrained(\"best_model\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"best_model\")\n",
    "\n",
    "class Task2TestDataset(Dataset):\n",
    "    def __init__(self, file_dir):\n",
    "        self.file_names = sorted(os.listdir(file_dir))\n",
    "        self.file_paths = [os.path.join(file_dir, fname) for fname in self.file_names]\n",
    "        self.file_ids = [os.path.splitext(fname)[0] for fname in self.file_names]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.file_paths[idx], \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read().strip()\n",
    "        return self.file_ids[idx], text  \n",
    "\n",
    "def test_collate_fn(batch, tokenizer, max_length=256):\n",
    "    file_ids, texts = zip(*batch)\n",
    "    encoded = tokenizer(\n",
    "        list(texts),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return {\n",
    "        \"file_ids\": list(file_ids),\n",
    "        \"input_ids\": encoded[\"input_ids\"],\n",
    "        \"attention_mask\": encoded[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "\n",
    "test_dataset = Task2TestDataset(r\"D:\\pythonProject\\Lab5_empty\\data\\task2\\test\")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=partial(test_collate_fn, tokenizer=tokenizer),\n",
    "    #Every time the num_workers not null will stuck, do not know why\n",
    "    num_workers=0,  \n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "\n",
    "model_task2.eval()\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm.tqdm(test_loader, desc=\"Predicting\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        file_ids = batch[\"file_ids\"]\n",
    "\n",
    "        logits = model_task2(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "\n",
    "        for fid, pred in zip(file_ids, preds):\n",
    "            results.append({\"file\": fid, \"predictions\": pred})\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df = df.sort_values(\"file\")  \n",
    "df = df[[\"predictions\"]]     \n",
    "df.to_csv(\"submission.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37185aff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*End of Task 2. Upload your final predictions (the file* `submission.csv` *) to* **Homework 2 - Code** *on* **NextIlearn**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d436d1",
   "metadata": {},
   "source": [
    "\\#\\# 3. Simple Autoregressive Extension of BERT\n",
    "\n",
    "BERT is not autoregressive by design. Here, we try to simulate next-token prediction using BERT with causal masking.\n",
    "\n",
    "Remember, that in the MLM pretraining task, BERT is trained to predict the most probable token corresponding to the input token `'[MASK]'`.\n",
    "\n",
    "E.g.: `'[CLS] The weather is [MASK]. [SEP]'` → **BERT** → `'[CLS] The weather is great. [SEP]'`\n",
    "\n",
    "We will use this to simulate autoregression, i.e. the generation of a text token by token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd80e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "model = AutoModelForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2472ab2c",
   "metadata": {},
   "source": [
    "An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743e18d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The capital of France is [MASK].\"\n",
    "\n",
    "# tokenize:\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# generate:\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75d3071",
   "metadata": {},
   "source": [
    "The shape of `logits` is:\n",
    "\n",
    "  *number of texts* $~\\times~$ *number of tokens*  $~\\times~$  *vocabulary size*\n",
    "\n",
    "→ This is a collection of token probabilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b956b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top prediction for masked token:\n",
    "predicted_token_id = logits[0, -3].argmax(axis=-1)\n",
    "\n",
    "print(\"Predicted token:\", tokenizer.decode(predicted_token_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ab32e5",
   "metadata": {},
   "source": [
    "Now let's make this a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3891e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "def complete_text(prompt:str, max_tokens:Optional[int]=None, model=model, tokenizer=tokenizer, device=device):\n",
    "  # use the whole context window if max_tokens not specified:\n",
    "  if max_tokens is None: max_tokens = tokenizer.model_max_length - len(tokenizer(prompt).input_ids)\n",
    "\n",
    "  # pad prompt with '[MASK]' tokens to tell BERT the number of tokens:\n",
    "  prompt += ' '.join(['[MASK]']*max_tokens)\n",
    "\n",
    "  # tokenize:\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "  # generate token probabilities:\n",
    "  with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "  # get top prediction for first masked token:\n",
    "  predicted_token_id = logits[0, -max_tokens-1].argmax(axis=-1).cpu().tolist()\n",
    "\n",
    "  text = tokenizer.decode(inputs.input_ids[0, 1:-max_tokens-1].cpu().tolist() + [predicted_token_id])\n",
    "\n",
    "  # end autoregression if max_tokens == 1:\n",
    "  if max_tokens == 1: return text\n",
    "\n",
    "  # end autoregression on '.' token:\n",
    "  if predicted_token_id == tokenizer.vocab['.']: return text\n",
    "\n",
    "  # end autoregression on [SEP] token:\n",
    "  if predicted_token_id == tokenizer.sep_token_id: return text\n",
    "\n",
    "  return complete_text(text, max_tokens=max_tokens-1, model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64691dbd",
   "metadata": {},
   "source": [
    "Let's see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c77c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_text(\"The capital of France is \", max_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d9148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_text(\"The capital of France is \", max_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86329eba",
   "metadata": {},
   "source": [
    "Another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d4d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_text(\"one plus one is equal to \", max_tokens=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3cafeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_text(\"one plus one is equal to \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
